# simulation/forecast_calibration_advanced.py
"""
Advanced Forecast Calibration & Error Reporting
P0 Priority: MAE reduction â‰¥10%

This module implements:
- Advanced forecast calibration with confidence intervals
- Error reporting and analysis
- Model performance tracking
- Automated calibration adjustments
- MAE reduction strategies
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
from django.utils import timezone
from django.db import models
from core.models import Company, User
from events.event_bus import event_bus
import uuid
import json

logger = logging.getLogger(__name__)

@dataclass
class ForecastError:
    """Forecast error record"""
    id: str
    timestamp: timezone.datetime
    model_name: str
    forecast_period: str
    actual_value: float
    predicted_value: float
    error: float
    absolute_error: float
    percentage_error: float
    confidence_interval_lower: float
    confidence_interval_upper: float
    within_confidence: bool

@dataclass
class CalibrationMetrics:
    """Calibration metrics for a model"""
    model_name: str
    mae: float
    mape: float
    rmse: float
    bias: float
    coverage_rate: float
    calibration_score: float
    improvement_potential: float

class AdvancedForecastCalibration:
    """
    Advanced forecast calibration with error reporting and MAE reduction
    """
    
    def __init__(self):
        self.error_history: List[ForecastError] = []
        self.calibration_metrics: Dict[str, CalibrationMetrics] = {}
        self.model_performance: Dict[str, Dict[str, Any]] = {}
        
    def record_forecast_error(self, model_name: str, forecast_period: str,
                            actual_value: float, predicted_value: float,
                            confidence_interval: Tuple[float, float],
                            company: Company) -> ForecastError:
        """Record a forecast error for analysis"""
        error = actual_value - predicted_value
        absolute_error = abs(error)
        percentage_error = (absolute_error / actual_value) * 100 if actual_value != 0 else 0
        
        ci_lower, ci_upper = confidence_interval
        within_confidence = ci_lower <= actual_value <= ci_upper
        
        forecast_error = ForecastError(
            id=str(uuid.uuid4()),
            timestamp=timezone.now(),
            model_name=model_name,
            forecast_period=forecast_period,
            actual_value=actual_value,
            predicted_value=predicted_value,
            error=error,
            absolute_error=absolute_error,
            percentage_error=percentage_error,
            confidence_interval_lower=ci_lower,
            confidence_interval_upper=ci_upper,
            within_confidence=within_confidence
        )
        
        self.error_history.append(forecast_error)
        
        # Publish error event
        event_bus.publish(
            event_type='FORECAST_ERROR_RECORDED',
            data={
                'error_id': forecast_error.id,
                'model_name': model_name,
                'forecast_period': forecast_period,
                'absolute_error': absolute_error,
                'percentage_error': percentage_error,
                'within_confidence': within_confidence
            },
            company_id=company.id
        )
        
        logger.info(f"Forecast error recorded for {model_name}: MAE={absolute_error:.2f}")
        return forecast_error
    
    def calculate_calibration_metrics(self, model_name: str, 
                                     lookback_days: int = 30) -> CalibrationMetrics:
        """Calculate comprehensive calibration metrics for a model"""
        # Filter errors for the model and time period
        cutoff_date = timezone.now() - timedelta(days=lookback_days)
        model_errors = [
            error for error in self.error_history
            if error.model_name == model_name and error.timestamp >= cutoff_date
        ]
        
        if not model_errors:
            return CalibrationMetrics(
                model_name=model_name,
                mae=0.0, mape=0.0, rmse=0.0, bias=0.0,
                coverage_rate=0.0, calibration_score=0.0, improvement_potential=0.0
            )
        
        # Calculate basic metrics
        absolute_errors = [error.absolute_error for error in model_errors]
        percentage_errors = [error.percentage_error for error in model_errors]
        errors = [error.error for error in model_errors]
        
        mae = np.mean(absolute_errors)
        mape = np.mean(percentage_errors)
        rmse = np.sqrt(np.mean([e**2 for e in errors]))
        bias = np.mean(errors)
        
        # Calculate coverage rate (percentage of actuals within confidence intervals)
        within_ci_count = sum(1 for error in model_errors if error.within_confidence)
        coverage_rate = within_ci_count / len(model_errors) if model_errors else 0
        
        # Calculate calibration score (combination of accuracy and calibration)
        accuracy_score = max(0, 1 - (mae / np.mean([error.actual_value for error in model_errors])))
        calibration_score = (accuracy_score + coverage_rate) / 2
        
        # Calculate improvement potential
        baseline_mae = self._get_baseline_mae(model_name)
        improvement_potential = max(0, (baseline_mae - mae) / baseline_mae) if baseline_mae > 0 else 0
        
        metrics = CalibrationMetrics(
            model_name=model_name,
            mae=mae,
            mape=mape,
            rmse=rmse,
            bias=bias,
            coverage_rate=coverage_rate,
            calibration_score=calibration_score,
            improvement_potential=improvement_potential
        )
        
        self.calibration_metrics[model_name] = metrics
        return metrics
    
    def _get_baseline_mae(self, model_name: str) -> float:
        """Get baseline MAE for comparison"""
        # This would typically come from historical data or a simple baseline model
        # For now, return a reasonable baseline
        return 1000.0  # Example baseline MAE
    
    def generate_error_report(self, model_name: str, 
                            lookback_days: int = 30) -> Dict[str, Any]:
        """Generate comprehensive error report for a model"""
        metrics = self.calculate_calibration_metrics(model_name, lookback_days)
        
        # Get recent errors
        cutoff_date = timezone.now() - timedelta(days=lookback_days)
        recent_errors = [
            error for error in self.error_history
            if error.model_name == model_name and error.timestamp >= cutoff_date
        ]
        
        # Analyze error patterns
        error_analysis = self._analyze_error_patterns(recent_errors)
        
        # Generate recommendations
        recommendations = self._generate_calibration_recommendations(metrics, error_analysis)
        
        return {
            "model_name": model_name,
            "period_days": lookback_days,
            "metrics": {
                "mae": metrics.mae,
                "mape": metrics.mape,
                "rmse": metrics.rmse,
                "bias": metrics.bias,
                "coverage_rate": metrics.coverage_rate,
                "calibration_score": metrics.calibration_score,
                "improvement_potential": metrics.improvement_potential
            },
            "error_analysis": error_analysis,
            "recommendations": recommendations,
            "total_errors": len(recent_errors),
            "mae_reduction_target": metrics.mae * 0.9,  # 10% reduction target
            "achievable_reduction": metrics.improvement_potential * metrics.mae
        }
    
    def _analyze_error_patterns(self, errors: List[ForecastError]) -> Dict[str, Any]:
        """Analyze error patterns for insights"""
        if not errors:
            return {"pattern": "no_data", "insights": []}
        
        # Time-based patterns
        errors_by_hour = {}
        for error in errors:
            hour = error.timestamp.hour
            errors_by_hour[hour] = errors_by_hour.get(hour, 0) + 1
        
        # Magnitude patterns
        large_errors = [e for e in errors if e.absolute_error > np.percentile([e.absolute_error for e in errors], 90)]
        small_errors = [e for e in errors if e.absolute_error < np.percentile([e.absolute_error for e in errors], 10)]
        
        # Bias patterns
        positive_errors = [e for e in errors if e.error > 0]
        negative_errors = [e for e in errors if e.error < 0]
        
        # Confidence interval patterns
        within_ci = [e for e in errors if e.within_confidence]
        outside_ci = [e for e in errors if not e.within_confidence]
        
        return {
            "total_errors": len(errors),
            "errors_by_hour": errors_by_hour,
            "large_errors_count": len(large_errors),
            "small_errors_count": len(small_errors),
            "positive_bias_count": len(positive_errors),
            "negative_bias_count": len(negative_errors),
            "within_ci_count": len(within_ci),
            "outside_ci_count": len(outside_ci),
            "bias_ratio": len(positive_errors) / len(errors) if errors else 0,
            "coverage_rate": len(within_ci) / len(errors) if errors else 0,
            "peak_error_hour": max(errors_by_hour.items(), key=lambda x: x[1])[0] if errors_by_hour else None
        }
    
    def _generate_calibration_recommendations(self, metrics: CalibrationMetrics, 
                                            error_analysis: Dict[str, Any]) -> List[str]:
        """Generate calibration recommendations based on metrics and analysis"""
        recommendations = []
        
        # MAE reduction recommendations
        if metrics.mae > 500:  # High MAE threshold
            recommendations.append("Consider model retraining with recent data")
            recommendations.append("Implement ensemble methods to reduce variance")
        
        if metrics.bias > 100:  # Positive bias
            recommendations.append("Model shows positive bias - consider bias correction")
        elif metrics.bias < -100:  # Negative bias
            recommendations.append("Model shows negative bias - consider bias correction")
        
        # Coverage rate recommendations
        if metrics.coverage_rate < 0.8:  # Low coverage
            recommendations.append("Confidence intervals too narrow - consider widening")
            recommendations.append("Implement uncertainty quantification methods")
        
        # Calibration score recommendations
        if metrics.calibration_score < 0.7:
            recommendations.append("Overall calibration poor - consider model architecture changes")
            recommendations.append("Implement recalibration techniques")
        
        # Pattern-based recommendations
        if error_analysis.get("bias_ratio", 0) > 0.7:
            recommendations.append("Strong positive bias pattern detected - investigate data quality")
        elif error_analysis.get("bias_ratio", 0) < 0.3:
            recommendations.append("Strong negative bias pattern detected - investigate data quality")
        
        if error_analysis.get("large_errors_count", 0) > error_analysis.get("total_errors", 1) * 0.2:
            recommendations.append("High frequency of large errors - consider outlier detection")
        
        return recommendations
    
    def apply_calibration_adjustments(self, model_name: str, 
                                    adjustments: Dict[str, Any]) -> bool:
        """Apply calibration adjustments to improve model performance"""
        try:
            # This would typically involve:
            # 1. Retraining the model with adjusted parameters
            # 2. Updating confidence interval calculations
            # 3. Adjusting bias correction factors
            # 4. Implementing ensemble weights
            
            logger.info(f"Applying calibration adjustments for {model_name}: {adjustments}")
            
            # Publish adjustment event
            event_bus.publish(
                event_type='CALIBRATION_ADJUSTMENTS_APPLIED',
                data={
                    'model_name': model_name,
                    'adjustments': adjustments,
                    'timestamp': timezone.now().isoformat()
                }
            )
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to apply calibration adjustments for {model_name}: {e}")
            return False
    
    def track_mae_reduction(self, model_name: str, 
                           target_reduction: float = 0.1) -> Dict[str, Any]:
        """Track MAE reduction progress towards target"""
        metrics = self.calibration_metrics.get(model_name)
        if not metrics:
            return {"status": "no_data", "message": "No calibration metrics available"}
        
        baseline_mae = self._get_baseline_mae(model_name)
        current_mae = metrics.mae
        target_mae = baseline_mae * (1 - target_reduction)
        
        reduction_achieved = (baseline_mae - current_mae) / baseline_mae
        reduction_needed = target_reduction - reduction_achieved
        
        return {
            "model_name": model_name,
            "baseline_mae": baseline_mae,
            "current_mae": current_mae,
            "target_mae": target_mae,
            "target_reduction": target_reduction,
            "reduction_achieved": reduction_achieved,
            "reduction_needed": max(0, reduction_needed),
            "target_met": reduction_achieved >= target_reduction,
            "improvement_potential": metrics.improvement_potential
        }
    
    def generate_calibration_report(self, company: Company, 
                                  models: List[str] = None) -> Dict[str, Any]:
        """Generate comprehensive calibration report for all models"""
        if models is None:
            models = list(set(error.model_name for error in self.error_history))
        
        model_reports = {}
        overall_metrics = {
            "total_models": len(models),
            "models_meeting_target": 0,
            "average_mae": 0,
            "average_calibration_score": 0
        }
        
        for model_name in models:
            report = self.generate_error_report(model_name)
            model_reports[model_name] = report
            
            # Update overall metrics
            if report["recommendations"]:  # Has recommendations means needs improvement
                overall_metrics["models_meeting_target"] += 1
            
            overall_metrics["average_mae"] += report["metrics"]["mae"]
            overall_metrics["average_calibration_score"] += report["metrics"]["calibration_score"]
        
        if models:
            overall_metrics["average_mae"] /= len(models)
            overall_metrics["average_calibration_score"] /= len(models)
        
        return {
            "company_id": str(company.id),
            "generated_at": timezone.now().isoformat(),
            "overall_metrics": overall_metrics,
            "model_reports": model_reports,
            "summary": {
                "total_errors_analyzed": len(self.error_history),
                "models_analyzed": len(models),
                "calibration_status": "good" if overall_metrics["average_calibration_score"] > 0.7 else "needs_improvement"
            }
        }

# Global instance
advanced_forecast_calibration = AdvancedForecastCalibration()
